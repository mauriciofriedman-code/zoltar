# backend/rag_pipeline.py

from typing import List, Dict, Any, Tuple
from backend.llm_loader import get_chat_llm
from backend.retrieve import get_retriever
from backend.prompt_teacher import build_teacher_prompt


def safe_response(llm, messages: List[Dict]) -> str:
    """Envuelve la invocaci√≥n al LLM y devuelve siempre texto plano."""
    try:
        response = llm.invoke(messages)
        if hasattr(response, "content"):
            return response.content
        if isinstance(response, dict) and "content" in response:
            return response["content"]
        return str(response)
    except Exception as e:
        print(f"[safe_response] Error: {e}")
        return f"Error al generar respuesta: {e}"


def _contexts_to_text_and_sources(
    contexts: List[Any],
    enumerate_blocks: bool = True,
    max_chars_total: int = 6000,
    max_chars_per_block: int = 1200,
) -> Tuple[str, List[str]]:
    """
    Convierte documentos recuperados en texto enumerado + lista de referencias.
    """
    parts, total = [], 0
    sources: List[str] = []

    for i, c in enumerate(contexts, start=1):
        md = getattr(c, "metadata", {}) or {}
        doc_id = md.get("doc_id", "N/A")
        title = md.get("title", md.get("source", "Desconocido"))
        authors = md.get("authors", "Autores desconocidos")
        page = md.get("page", "N/A")
        source = md.get("source", "Fuente desconocida")

        block_txt = getattr(c, "page_content", "") or ""
        if len(block_txt) > max_chars_per_block:
            block_txt = block_txt[:max_chars_per_block].rsplit(" ", 1)[0] + "‚Ä¶"

        header = (
            f"T√≠tulo: {title}\n"
            f"Autores: {authors}\n"
            f"P√°gina: {page}\n"
            f"Fuente: {source}"
        )

        if enumerate_blocks:
            body = f"[{i}] {header}\nContenido:\n{block_txt}"
        else:
            body = f"{header}\nContenido:\n{block_txt}"

        if total + len(body) > max_chars_total:
            break

        parts.append(body)
        total += len(body)

        ref = f"{title} (p.{page})"
        if ref not in sources:
            sources.append(ref)

    return "\n\n".join(parts).strip(), sources


FALLBACK_NO_CONTEXT = (
    "Soy un asistente educativo con acceso a documentos. "
    "No encontr√© fragmentos relevantes para tu consulta.\n\n"
    "üëâ Puedes intentar con preguntas m√°s espec√≠ficas como:\n"
    "- '¬øC√≥mo se puede usar la IA en evaluaci√≥n formativa?'\n"
    "- 'Ventajas de los chatbots educativos en secundaria'\n\n"
    "Mis respuestas se basan exclusivamente en los documentos cargados."
)


def answer_with_rag(
    question: str,
    system_prompt: str = "",
    k: int = 5,
    allow_fallback: bool = True
) -> Dict:
    """
    Devuelve un dict con:
    - text: respuesta del modelo
    - sources: lista de referencias citadas
    """
    llm = get_chat_llm()

    try:
        retriever = get_retriever(k=k)
        contexts = retriever.invoke(question) or []
    except Exception:
        if not allow_fallback:
            return {"text": "No se pudo recuperar informaci√≥n desde el √≠ndice.", "sources": []}
        return {"text": FALLBACK_NO_CONTEXT, "sources": []}

    if not contexts:
        return {"text": FALLBACK_NO_CONTEXT, "sources": []}

    context_text, sources = _contexts_to_text_and_sources(contexts)

    prompt = (
        system_prompt or
        "Eres un experto en tecnolog√≠a educativa.\n"
        "Responde preguntas sobre el uso de la inteligencia artificial y la generaci√≥n artificial (GenIA) en educaci√≥n.\n"
        "Usa solamente la informaci√≥n proporcionada en el contexto.\n"
        "No inv
























